# -*- coding: utf-8 -*-
"""test_logistic_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D91wE5kGBnT-xGySxPh22OjrMEAUhLg8
"""

import os
import re
import nltk
import torch
import string
import kagglehub
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
from itertools import product
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')
nltk.download('punkt_tab')

"""# 1. Load the data"""

path = kagglehub.dataset_download("danofer/sarcasm")

print("Path to dataset files:", path)

df = pd.read_csv(f"{path}/train-balanced-sarcasm.csv")

df.head()

"""# 2. Data Preprocessing"""

# Check for top 5 subreddits in terms of sample counts
top_5_subreddits = df['subreddit'].value_counts().head(5)

top_5_subreddits

df = df[df['subreddit'].isin(["AskReddit", "politics", "worldnews", "leagueoflegends", "pcmasterrace"])]

df.head()

# Handle missing values
df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

# First split: 60% for train, 40% for val+test
train_df, val_test_df = train_test_split(df, test_size=0.4, random_state=42)

# Second split: 50% of val+test for val, 50% for test
val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)

"""# 3. Test (Logistic Regression)"""

def convert_emoticons(text):

  # Define emoticon mapping
  emoticon_map = {
    ":)": "[HAPPY]",
    ":-)": "[HAPPY]",
    ":D": "[LAUGH]",
    "XD": "[LAUGH]",
    ":(": "[SAD]",
    ":-(": "[SAD]",
    ":/": "[SKEPTICAL]",
    ":\\": "[SKEPTICAL]",
    ":P": "[PLAYFUL]",
    ":-P": "[PLAYFUL]",
    ";)": "[WINK]",
    ";-)": "[WINK]",
    "<3": "[LOVE]",
    ":O": "[SURPRISE]",
    ":-O": "[SURPRISE]",
    ">:(": "[ANGRY]",
    ">:-(": "[ANGRY]",
  }

  for emoticon, tag in emoticon_map.items():
    text = re.sub(re.escape(emoticon), tag, text)

  return text

def lemmatize_text(text):
  lemmatizer = WordNetLemmatizer()
  tokens = word_tokenize(text)
  lemmas = [lemmatizer.lemmatize(token) for token in tokens]
  return " ".join(lemmas)

def remove_stopwords(text):
  stop_words = set(stopwords.words('english'))
  words = text.split()
  filtered_words = [word for word in words if word.lower() not in stop_words]
  return " ".join(filtered_words)

def remove_punctuation(text):
  return text.translate(str.maketrans('', '', string.punctuation))

"""Parameters: {'emoticons_replacement': True, 'lemmatization': False, 'stopwords_removal': False, 'punctuation_removal': True, 'case_normalization': True, 'ngram_range': (1, 2), 'max_features': None, 'vectorizer': 'count', 'C': 0.1}"""

def test_logistic_regression(df_train, df_test):

  """ Performance metrics for Logistic Regression on the test set """

  df_train_copy = df_train.copy()
  df_test_copy = df_test.copy()

  # Emoticons
  df_train_copy['comment'] = df_train_copy['comment'].apply(convert_emoticons)
  df_test_copy['comment'] = df_test_copy['comment'].apply(convert_emoticons)

  vectorizer_params = {
    'max_features': None,
    'ngram_range': (1, 2),
    'lowercase': True,
    'strip_accents': 'unicode',
    'preprocessor': remove_punctuation
  }

  vectorizer = CountVectorizer(**vectorizer_params)

  # Transform the text data
  X_train = vectorizer.fit_transform(df_train_copy['comment'])
  X_test = vectorizer.transform(df_test_copy['comment'])

  y_train = df_train_copy['label']
  y_test = df_test_copy['label']

  # Train logistic regression
  model = LogisticRegression(
    C=0.1,
    max_iter=1000,
    random_state=42
  )
  model.fit(X_train, y_train)

  # Evaluate
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  test_f1 = f1_score(y_test, y_pred)

  print(f"Accuracy: {accuracy}")
  print(f"Precision: {precision}")
  print(f"Recall: {recall}")
  print(f"F1 Score: {test_f1}")

  results = {}

  for subreddit in df_test['subreddit'].unique():
    df_test_subreddit = df_test[df_test['subreddit'] == subreddit]
    X_test_subreddit = vectorizer.transform(df_test_subreddit['comment'])
    y_test_subreddit = df_test_subreddit['label']

    # Get predictions
    y_pred_subreddit = model.predict(X_test_subreddit)

    # Calculate metrics
    accuracy = accuracy_score(y_test_subreddit, y_pred_subreddit)
    f1 = f1_score(y_test_subreddit, y_pred_subreddit)

    results[subreddit] = {'accuracy': accuracy, 'f1': f1}

  return results

results = test_logistic_regression(train_df, test_df)

results

subreddits = list(results.keys())
accuracy = [results[subreddit]['accuracy'] for subreddit in subreddits]
f1 = [results[subreddit]['f1'] for subreddit in subreddits]
x = np.arange(len(subreddits))  # the label locations
width = 0.35  # the width of the bars
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, accuracy, width, label='Accuracy')
rects2 = ax.bar(x + width/2, f1, width, label='F1 Score')
# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Performance Metrics per Subreddit')
ax.set_xticks(x)
ax.set_xticklabels(subreddits)
ax.legend()
fig.tight_layout()
plt.show()