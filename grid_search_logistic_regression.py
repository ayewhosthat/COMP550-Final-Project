# -*- coding: utf-8 -*-
"""grid_search_logistic_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zbCHN2hePr8nJG8LdlG174J2S6ThIFKy
"""

import os
import re
import nltk
import torch
import string
import kagglehub
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
from itertools import product
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.linear_model import LogisticRegressiontlk;jb
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')
nltk.download('punkt_tab')

"""# 1. Load the data"""

path = kagglehub.dataset_download("danofer/sarcasm")

print("Path to dataset files:", path)

df = pd.read_csv(f"{path}/train-balanced-sarcasm.csv")

df.head()

"""# 2. Data Preprocessing"""

# Check for top 5 subreddits in terms of sample counts
top_5_subreddits = df['subreddit'].value_counts().head(5)

top_5_subreddits

df = df[df['subreddit'].isin(["AskReddit", "politics", "worldnews", "leagueoflegends", "pcmasterrace"])]

df.head()

# Handle missing values
df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

# First split: 60% for train, 40% for val+test
train_df, val_test_df = train_test_split(df, test_size=0.4, random_state=42)

# Second split: 50% of val+test for val, 50% for test
val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)

"""# 3. Grid Search (Logistic Regression)"""

def convert_emoticons(text):

  # Define emoticon mapping
  emoticon_map = {
    ":)": "[HAPPY]",
    ":-)": "[HAPPY]",
    ":D": "[LAUGH]",
    "XD": "[LAUGH]",
    ":(": "[SAD]",
    ":-(": "[SAD]",
    ":/": "[SKEPTICAL]",
    ":\\": "[SKEPTICAL]",
    ":P": "[PLAYFUL]",
    ":-P": "[PLAYFUL]",
    ";)": "[WINK]",
    ";-)": "[WINK]",
    "<3": "[LOVE]",
    ":O": "[SURPRISE]",
    ":-O": "[SURPRISE]",
    ">:(": "[ANGRY]",
    ">:-(": "[ANGRY]",
  }

  for emoticon, tag in emoticon_map.items():
    text = re.sub(re.escape(emoticon), tag, text)

  return text

def lemmatize_text(text):
  lemmatizer = WordNetLemmatizer()
  tokens = word_tokenize(text)
  lemmas = [lemmatizer.lemmatize(token) for token in tokens]
  return " ".join(lemmas)

def remove_stopwords(text):
  stop_words = set(stopwords.words('english'))
  words = text.split()
  filtered_words = [word for word in words if word.lower() not in stop_words]
  return " ".join(filtered_words)

def remove_punctuation(text):
  return text.translate(str.maketrans('', '', string.punctuation))

def logistic_regression_grid_search(df_train, df_val, resume_at_index=0):

  """ Preprocessing + Hyperparameter Tuning for Logistic Regression """

  params = {
    'emoticons': [True, False],
    'lemmatization': [True, False],
    'stopwords': [True, False],
    'punctuation': [True, False],                     # Vectorizer
    'lowercase': [True, False],                       # Vectorizer
    'ngram_range': [(1, 1), (1, 2), (2, 2)],          # Vectorizer
    'max_features': [250, 1000, 2500, 10000, None],   # Vectorizer
    'vectorizer': ['count', 'tfidf'],                 # Vectorizer
    'C': [0.1, 1, 10]
  }

  # Generate all combinations
  comb_keys, comb_values = zip(*params.items())
  search_combinations = [dict(zip(comb_keys, v)) for v in product(*comb_values)]
  best_accuracy = 0
  best_params = {}

  # Resume from a specific index if needed
  for idx, params in enumerate(search_combinations[resume_at_index:], start=resume_at_index):
    try:

      # Copy the dataframes
      df_train_copy = df_train.copy()
      df_val_copy = df_val.copy()

      # Apply preprocessing based on parameters
      if params['emoticons']:
        df_train_copy['comment'] = df_train_copy['comment'].apply(convert_emoticons)
        df_val_copy['comment'] = df_val_copy['comment'].apply(convert_emoticons)

      if params['lemmatization']:
        df_train_copy['comment'] = df_train_copy['comment'].apply(lemmatize_text)
        df_val_copy['comment'] = df_val_copy['comment'].apply(lemmatize_text)

      if params['stopwords']:
        df_train_copy['comment'] = df_train_copy['comment'].apply(remove_stopwords)
        df_val_copy['comment'] = df_val_copy['comment'].apply(remove_stopwords)

      # Initialize vectorizer based on parameters
      vectorizer_params = {
        'max_features': params['max_features'],
        'ngram_range': params['ngram_range'],
        'lowercase': params['lowercase'],
        'strip_accents': 'unicode',
        'preprocessor': None if params['punctuation'] else remove_punctuation
      }

      if params['vectorizer'] == 'count':
        vectorizer = CountVectorizer(**vectorizer_params)

      if params['vectorizer'] == 'tfidf':
        vectorizer = TfidfVectorizer(**vectorizer_params)

      # Transform the text data
      X_train = vectorizer.fit_transform(df_train_copy['comment'])
      X_val = vectorizer.transform(df_val_copy['comment'])

      y_train = df_train_copy['label']
      y_val = df_val_copy['label']

      # Train logistic regression
      model = LogisticRegression(
        C=params['C'],
        max_iter=1000,
        random_state=42
      )
      model.fit(X_train, y_train)

      # Evaluate
      val_accuracy = model.score(X_val, y_val)

      # Update best parameters if current model is better
      if val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        best_params = params.copy()

      # Print progress
      print(f"Combination {idx + 1}/{len(search_combinations)}")
      print(f"Parameters: {params}")
      print(f"Validation Accuracy: {val_accuracy:.4f}")
      print(f"Validation Accuracy: {val_accuracy:.4f}")
      print(f"Best Accuracy So Far: {best_accuracy:.4f}")
      print("-" * 50)

    except Exception as e:
      print(f"Error in combination {idx}: {str(e)}")
      continue

  return best_params, best_accuracy

logistic_regression_grid_search(train_df, val_df)

"""Parameters: {'emoticons': True, 'lemmatization': False, 'stopwords': False, 'punctuation': True, 'lowercase': True, 'ngram_range': (1, 2), 'max_features': None, 'vectorizer': 'count', 'C': 0.1}"""