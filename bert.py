# -*- coding: utf-8 -*-
"""bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dxxk_mUw2eNM7zxd_CwjgIKB9CFTHMJ4
"""

import os
import torch
import kagglehub
import numpy as np
import pandas as pd
import seaborn as sns
import torch.nn as nn
from tqdm import tqdm
import torch.optim as optim
import matplotlib.pyplot as plt
from huggingface_hub import notebook_login
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

!git config --global credential.helper store

!git config --global credential.helper

!huggingface-cli login

notebook_login()

"""# 1. Load the data"""

path = kagglehub.dataset_download("danofer/sarcasm")

print("Path to dataset files:", path)

df = pd.read_csv(f"{path}/train-balanced-sarcasm.csv")

df.head()

"""# 2. Data Preprocessing"""

# Check for top 5 subreddits in terms of sample counts
top_5_subreddits = df['subreddit'].value_counts().head(5)

top_5_subreddits

df = df[df['subreddit'].isin(["AskReddit", "politics", "worldnews", "leagueoflegends", "pcmasterrace"])]

df.head()

# Handle missing values
df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

"""# 3. Finetune BERT"""

df.rename(columns={'comment': 'text'}, inplace=True)

"""Convert dataframe to Pytorch Dataloader"""

class SarcasmDataset(Dataset):

  """ Dataset for sarcasm detection with BERT """

  def __init__(self, data, tokenizer, max_len=512):
    self.data = data
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index):
    # Get text and label at index
    text = str(self.data.iloc[index]['text'])
    label = self.data.iloc[index]['label']
    subreddit = self.data.iloc[index]['subreddit']

    # Encode the text
    encoding = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,  # Add [CLS] and [SEP]
      max_length=self.max_len,
      padding='max_length',     # Pad to max_length
      truncation=True,          # Truncate to max_length
      return_attention_mask=True,
      return_tensors='pt'       # Return PyTorch tensors
    )

    return {
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'labels': torch.tensor(label, dtype=torch.long),
      'subreddit': subreddit
    }

"""Train/Val/Test Split (60/20/20)"""

# First split: 60% for train, 40% for val+test
train_df, val_test_df = train_test_split(df, test_size=0.4, random_state=42)

# Second split: 50% of val+test for val, 50% for test
val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Create dataset instances
train_dataset = SarcasmDataset(train_df, tokenizer, max_len=512)
val_dataset = SarcasmDataset(val_df, tokenizer, max_len=512)
test_dataset = SarcasmDataset(test_df, tokenizer, max_len=512)

# Create dataloaders
train_dataloader = DataLoader(
    train_dataset,  # Use dataset instance, not DataFrame
    batch_size=32,
    shuffle=True
)
val_dataloader = DataLoader(
    val_dataset,    # Use dataset instance, not DataFrame
    batch_size=32,
    shuffle=False
)
test_dataloader = DataLoader(
    test_dataset,   # Use dataset instance, not DataFrame
    batch_size=32,
    shuffle=False
)

len(train_dataloader), len(val_dataloader), len(test_dataloader)

def finetune_bert(train_dataloader, validation_dataloader, epochs=4, lr=2e-5, max_len=512):

  """ Finetune BERT model for sarcasm detection """

  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
  save_path=f"BERT_sarcasm_lr_{lr}"
  print(f"Saving model to: {save_path}")

  # Find the device
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  # Setup BERT
  model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
  )
  model.to(device)

  optimizer = AdamW(model.parameters(), lr=lr)

  for epoch_num in range(epochs):
    print(f'Epoch: {epoch_num + 1}/{epochs}')

    # Training loop with progress bar
    model.train()
    train_loss = 0
    train_preds, train_labels = [], []

    for step_num, batch_data in tqdm(enumerate(train_dataloader), desc="Training", total=len(train_dataloader)):
      b_input_ids = batch_data['input_ids'].to(device)
      b_input_mask = batch_data['attention_mask'].to(device)
      b_labels = batch_data['labels'].to(device)

      model.zero_grad()
      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
      loss = outputs[0]
      train_loss += loss.item() # Accumulate training loss
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
      optimizer.step()

      # Accumulate predictions and labels for training metrics
      logits = outputs[1]
      _, preds = torch.max(logits, dim=1)
      train_preds.extend(preds.cpu().numpy())
      train_labels.extend(b_labels.cpu().numpy())

    avg_train_loss = train_loss / len(train_dataloader) # Calculate average training loss
    print(f"Average training loss: {avg_train_loss}")

    # Calculate and print training metrics
    train_accuracy = accuracy_score(train_labels, train_preds)
    train_precision = precision_score(train_labels, train_preds)
    train_recall = recall_score(train_labels, train_preds)
    train_f1 = f1_score(train_labels, train_preds)

    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Training Precision: {train_precision:.4f}")
    print(f"Training Recall: {train_recall:.4f}")
    print(f"Training F1 Score: {train_f1:.4f}")

    # Validation loop
    model.eval()
    val_loss = 0
    val_preds, val_labels = [], []

    # Disable gradient calculation during validation
    with torch.no_grad():
      for batch_data in tqdm(validation_dataloader, desc="Validation", total=len(validation_dataloader)):
        b_input_ids = batch_data['input_ids'].to(device)
        b_input_mask = batch_data['attention_mask'].to(device)
        b_labels = batch_data['labels'].to(device)

        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs[0]
        val_loss += loss.item()

        # Calculate accuracy
        logits = outputs[1]
        _, preds = torch.max(logits, dim=1)
        val_preds.extend(preds.cpu().numpy())
        val_labels.extend(b_labels.cpu().numpy())

    avg_val_loss = val_loss / len(validation_dataloader)
    print(f"Average validation loss: {avg_val_loss}")

    # Calculate and print validation metrics
    val_accuracy = accuracy_score(val_labels, val_preds)
    val_precision = precision_score(val_labels, val_preds)
    val_recall = recall_score(val_labels, val_preds)
    val_f1 = f1_score(val_labels, val_preds)

    print(f"Validation Accuracy: {val_accuracy:.4f}")
    print(f"Validation Precision: {val_precision:.4f}")
    print(f"Validation Recall: {val_recall:.4f}")
    print(f"Validation F1 Score: {val_f1:.4f}")

  model.save_pretrained(save_path)
  tokenizer.save_pretrained(save_path)

  model.push_to_hub(save_path)
  tokenizer.push_to_hub(save_path)

  return model, tokenizer

model2e05, tokenizer2e05 = finetune_bert(
  train_dataloader,
  val_dataloader,
  epochs=4,
  lr=2e-5,
  max_len=512
)

model3e05, tokenizer3e05 = finetune_bert(
  train_dataloader,
  val_dataloader,
  epochs=4,
  lr=3e-5,
  max_len=512
)

model5e05, tokenizer5e05 = finetune_bert(
  train_dataloader,
  val_dataloader,
  epochs=4,
  lr=5e-5,
  max_len=512
)

"""# 4. Test BERT"""

def compute_performance_metrics(labels, predictions):

  """ Compute metrics for BERT model """

  # Compute metrics
  accuracy = accuracy_score(labels, predictions)
  precision = precision_score(labels, predictions, average='macro', zero_division=1)
  recall = recall_score(labels, predictions, average='macro', zero_division=1)
  f1 = f1_score(labels, predictions, average='macro', zero_division=1)

  # Print metrics
  print(f"Accuracy: {accuracy:.4f}")
  print(f"Precision: {precision:.4f}")
  print(f"Recall: {recall:.4f}")
  print(f"F1 Score: {f1:.4f}")

  return accuracy, precision, recall, f1

def bert_test_metrics(model_name, test_dataloader):

  """ Test BERT model on test set and print metrics """

  # Get the GPU
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  # Get the model and the tokenizer
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForSequenceClassification.from_pretrained(model_name)
  model.eval()
  model.to(device)

  # Initialize metrics
  test_predictions = []
  test_labels = []

  # Evaluation Loop
  with torch.no_grad():
    for batch in test_dataloader:
      # Move data to device
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['labels'].to(device)

      # Model forward pass
      outputs = model(input_ids=input_ids, attention_mask=attention_mask)
      logits = outputs.logits
      predictions = torch.argmax(logits, dim=-1)

      # Collect predictions and true labels
      test_predictions.extend(predictions.cpu().numpy())
      test_labels.extend(labels.cpu().numpy())

  accuracy, precision, recall, f1 = compute_performance_metrics(test_labels, test_predictions)

  return accuracy, precision, recall, f1

bert_test_metrics('saintsauce/BERT_sarcasm_lr_3e-05', test_dataloader)

def subreddit_metrics(model_name, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    model.eval()
    model.to(device)

    subreddit_predictions = {}  # Store predictions for each subreddit
    subreddit_labels = {}      # Store labels for each subreddit

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            subreddits = batch['subreddit']  # Get subreddits for this batch

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            # Store predictions and labels by subreddit
            for i, subreddit in enumerate(subreddits):
                subreddit_predictions.setdefault(subreddit, []).append(predictions[i].item())
                subreddit_labels.setdefault(subreddit, []).append(labels[i].item())

    # Calculate accuracy and F1 score for each subreddit
    subreddit_accuracies = {}
    subreddit_f1_scores = {}
    for subreddit in subreddit_predictions:
        accuracy = accuracy_score(subreddit_labels[subreddit], subreddit_predictions[subreddit])
        f1 = f1_score(subreddit_labels[subreddit], subreddit_predictions[subreddit]) # Calculate F1 score
        subreddit_accuracies[subreddit] = accuracy
        subreddit_f1_scores[subreddit] = f1  # Store F1 score

    # Print accuracies and F1 scores
    for subreddit, accuracy in subreddit_accuracies.items():
        f1 = subreddit_f1_scores[subreddit] # Get F1 score for the subreddit
        print(f"Subreddit: {subreddit}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}")

    return subreddit_accuracies, subreddit_f1_scores  # Return both dictionaries

subreddit_accuracies, subreddit_f1_scores = subreddit_metrics('saintsauce/BERT_sarcasm_lr_3e-05', test_dataloader)

subreddits = list(subreddit_accuracies.keys())
accuracy = [subreddit_accuracies[subreddit] for subreddit in subreddits]
f1 = [subreddit_f1_scores[subreddit] for subreddit in subreddits]

x = np.arange(len(subreddits))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, accuracy, width, label='Accuracy')
rects2 = ax.bar(x + width/2, f1, width, label='F1 Score')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Performance Metrics per Subreddit')
ax.set_xticks(x)
ax.set_xticklabels(subreddits)
ax.legend()

fig.tight_layout()
plt.show()